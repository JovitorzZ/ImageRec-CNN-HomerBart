{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JovitorzZ/ImageRec-CNN-HomerBart/blob/main/ImageRec_CNN_HomerBart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN para reconhecimento de imagens**\n",
        "\n",
        "**Definição de Problema:**\n",
        "\n",
        "A FOX deseja automatizar a identificação de personagens em cenas de episódios da série *Os Simpsons*, focando especificamente nos personagens Homer e Bart. Com milhares de episódios e uma vasta quantidade de cenas e frames, realizar esse processo manualmente é impraticável, demorado e propenso a erros humanos, o que limita a eficiência e a precisão na catalogação e análise dos conteúdos. Para resolver esse desafio, a FOX solicita ao seu time de tecnologia a implementação de uma rede neural convolucional (CNN) capaz de identificar automaticamente imagens dos personagens Homer e Bart.\n",
        "\n",
        "\n",
        "---\n",
        "**Objetivo do Projeto:**\n",
        "\n",
        "Essa classificação automática permitirá a FOX:\n",
        "- Criar conteúdo exclusivo focado nos personagens mais populares, potencializando oportunidades de merchandising e melhorando o engajamento do público.\n",
        "- Oferecer uma experiência interativa e personalizada aos fãs, permitindo buscas avançadas e navegação por cenas específicas de seus personagens favoritos.\n",
        "- Indexar e catalogar cenas de maneira automatizada, facilitando a criação de playlists temáticas e catálogos, como \"momentos clássicos do Homer\" ou \"as maiores travessuras de Bart\".\n",
        "\n",
        "\n",
        "---\n",
        "Ao implementar a CNN, a FOX visa não apenas otimizar processos internos e reduzir o tempo de trabalho, mas também expandir seu impacto no mercado de entretenimento ao proporcionar uma experiência de visualização inovadora e adaptada às preferências dos espectadores."
      ],
      "metadata": {
        "id": "_sX1-pYxTR6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1° Passo:  Fazer o Download/Import das bibliotecas"
      ],
      "metadata": {
        "id": "jkm68GTcV3DU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este projeto iremos utilizar as versões:\n",
        "* 2.16.1 do **TensorFlow**\n",
        "* 1.26.4 do **Numpy**\n",
        "* 5.5.0 do **Gradio**"
      ],
      "metadata": {
        "id": "wlulnV56oMpd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8prG0tk88ZIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535feec5-d824-49d6-ca28-1ced29f5960e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.16.1 -q\n",
        "!pip install numpy==1.26.4 -q\n",
        "!pip install gradio==5.5.0 -q\n",
        "#!pip install tempfile -q\n",
        "#!pip install zipfile -q\n",
        "#!pip install PIL -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importando as bibliotecas\n",
        "import numpy as np #Biblioteca para manipulação de arrays(vetores)\n",
        "import tensorflow as tf #Biblioteca para rede neurais (Deep Learning)\n",
        "import tempfile #Biblioteca para criar pastas temporarias\n",
        "import zipfile  #Biblioteca para descompactar uma pasta zip\n",
        "import gradio as gr #Biblioteca para criar uma interface grafica para o usuario\n",
        "from PIL import Image  # Biblioteca Pillow para manipulação de imagem"
      ],
      "metadata": {
        "id": "hfwFgHON0RKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importações específicas para construir e treinar o modelo de rede neural\n",
        "from tensorflow.keras.preprocessing.image  import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, InputLayer\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "wn1Dij450wEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **ImageDataGenerator**: Responsável por fazer a geração de novas imagens a partir das fotos iniciais e consequentimente aumentar o conjuto de dados.\n",
        "* **Sequential**: Responsável por criar um modelo onde as camadas são empilhadas uma após a outra, como uma \"pilha\" linear de camadas.\n",
        "* **Conv2D**: é uma camada de convolução que detecta características (como bordas) nas imagens.\n",
        "* **MaxPooling2D**: Reduz o tamanho das imagens, mantendo apenas as informações principais (ajuda a simplificar o modelo).\n",
        "* **Flatten**: Transforma as imagens em uma única linha de dados, preparando-as para as próximas camadas.\n",
        "* **Dense**: é uma camada \"densa\" onde cada neurônio se conecta com todos os outros da camada seguinte, útil para tomar decisões.\n",
        "* **Dropout**: \"desativa\" alguns neurônios aleatoriamente durante o treinamento, ajudando a evitar que o modelo memorize (overfitting).\n",
        "* **BatchNormalization**: normaliza a saída de uma camada para ajudar a acelerar e estabilizar o treinamento.\n",
        "* **InputLayer**: define o formato da entrada para o modelo, útil para especificar o tamanho da imagem de entrada.\n",
        "* **Image**: Permite carregar e pré-processar imagens individuais para teste ou inferência, útil ao usar imagens fora do conjunto de dados principal.\n",
        "* **EarlyStopping**: Interrompe o treinamento quando o modelo para de melhorar, evitando overfitting e economizando tempo.\n"
      ],
      "metadata": {
        "id": "r-e36mmd1-Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2°Passo: Carregamento e Pré-processamento das imagens"
      ],
      "metadata": {
        "id": "Nfgn_1m0CZhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Carregamento do dataset"
      ],
      "metadata": {
        "id": "evr9qSPWQDtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando pasta temporaria\n",
        "temp_dir = tempfile.TemporaryDirectory()\n",
        "print(temp_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu-IlU4qAEKH",
        "outputId": "7c9df0d0-9f80-4109-823f-b970494ffd63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<TemporaryDirectory '/tmp/tmpxtlhqeyo'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lendo o arquivo dataset_personagens e extraindo para uma pasta temporária com o mesmo nome\n",
        "with zipfile.ZipFile('/content/dataset_personagens.zip', 'r') as zip_ref: #Informar o caminho do seu dataset\n",
        "  zip_ref.extractall(temp_dir.name)"
      ],
      "metadata": {
        "id": "88U9Gk7y944_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Gerando novas imagens a partir das que já contém no dataset"
      ],
      "metadata": {
        "id": "I6ZmovZJU408"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gerador_treinamento = ImageDataGenerator(rescale = 1/255, #Normalizando os valores do pixels das imagens\n",
        "                                         rotation_range = 360, #Rotação aleatória das imagens no intervalo de -360 a +360 graus\n",
        "                                         width_shift_range = 0.3, #Deslocamento aleatório horizontal\n",
        "                                         height_shift_range = 0.3, #Deslocamento aleatório vertical\n",
        "                                         brightness_range = (0.1, 0.9), #Alteração aleatória em um range de 10% até 90% da luminosidade\n",
        "                                         horizontal_flip = True, #Realiza a inversao horizontal, espelhamento da imagem\n",
        "                                         vertical_flip = True, #Realiza a inversao vertical, espelhamento da imagem\n",
        "                                         shear_range = 0.2, #realiza o deslocamento da imagem criando distorcao\n",
        "                                         zoom_range = 0.5 #aplica zoom\n",
        ")"
      ],
      "metadata": {
        "id": "zv-u9M6gCBEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salvando a base de treino criada apartir das novas imagens na pasta temporária: /dataset_personagens/training_set\n",
        "\n",
        "Com seu tamanho sendo 64x64 pixels e proceesando as em blocos de 32 imagens, por ser somente imagens do Bart e do Homer será uma classificação Binaria: 1 para o **Homer** e 0 para o **Bart**.\n"
      ],
      "metadata": {
        "id": "PKbH-LL-VxXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_treino = gerador_treinamento.flow_from_directory(f'{temp_dir.name}/dataset_personagens/training_set',\n",
        "                                                target_size = (64,64),\n",
        "                                                batch_size = 32,\n",
        "                                                class_mode = 'binary' #Se fosse um problema onde serão identificado mais personagens o class_mode deve ser trocado para 'categorical'\n",
        "                                                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8ZLL6CFFdNz",
        "outputId": "b767266f-e77a-42ac-8199-510ae03515c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 196 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalizando os valores do pixels das imagens de teste\n",
        "gerador_teste = ImageDataGenerator(rescale = 1/255)"
      ],
      "metadata": {
        "id": "Rp2eMjcMFTHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Executando o mesmo processo da base_treino na base_teste\n",
        "base_teste = gerador_teste.flow_from_directory(f'{temp_dir.name}/dataset_personagens/test_set',\n",
        "                                                target_size = (64,64),\n",
        "                                                batch_size = 32,\n",
        "                                                class_mode = 'binary'\n",
        "                                                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4piQtjJGnuu",
        "outputId": "f8915135-f157-40fc-fbdf-07f44cbcae18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 73 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3°Passo: Treinamento da Rede Neural Convolucional"
      ],
      "metadata": {
        "id": "b0IQDgFEHISa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN sequencial**: É uma arquitetura de rede neural onde as camadas são empilhadas de forma linear, uma após a outra."
      ],
      "metadata": {
        "id": "WhlmVKrWtNF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = Sequential()\n",
        "\n",
        "#Definido camada de entrada do modelo\n",
        "cnn.add(InputLayer(input_shape = (64,64,3)))  #Espera imagens de tamanho 64x64 com 3 canais (RGB)\n",
        "\n",
        "#Primeira camada de convolução\n",
        "cnn.add(Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu')) #Aplica 32 filtros de tamanho 3x3 à imagem de entrada, utilizando a função de ativação ReLU\n",
        "cnn.add(BatchNormalization()) #Normaliza a saída da camada anterior para acelerar o treinamento\n",
        "cnn.add(MaxPooling2D(pool_size = (2,2))) #Reduz pela metade a resolução, mantendo as características principais\n",
        "\n",
        "#Segunda camada de convolução, semelhante à primeira, para aprofundar a extração de características\n",
        "#Troquei a função de ativação para Laeky_relu para ajudar a evitar o problema da \"morte dos neurônios\" que pode ocorrer com ReLU, permitindo que pequenos valores negativos passem\n",
        "cnn.add(Conv2D(filters = 32, kernel_size = (3,3), activation = 'leaky_relu'))\n",
        "cnn.add(BatchNormalization())\n",
        "cnn.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "#Achata a saída das camadas anteriores (matriz 2D) em um vetor 1D\n",
        "cnn.add(Flatten())\n",
        "\n",
        "#Camada Densa\n",
        "cnn.add(Dense(units = 128, activation = 'relu'))#Com 128 unidades e função de ativação ReLU\n",
        "cnn.add(Dropout(0.3)) #Dropout para prevenir overfitting, desativando aleatoriamente 30% dos neurônios durante o treinamento\n",
        "\n",
        "#Camada de saída com um único neurônio e função de ativação sigmoide para classificação binária\n",
        "cnn.add(Dense(units = 1, activation = 'sigmoid'))"
      ],
      "metadata": {
        "id": "n4STWSgaJBdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c2ed07-adc6-4a2a-f144-d868a831beea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "9XlexAmLJ1Bb",
        "outputId": "96bc631e-7e59-424f-ecce-e0676392f15a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m9,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6272\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m802,944\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6272</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">802,944</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m813,473\u001b[0m (3.10 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">813,473</span> (3.10 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m813,345\u001b[0m (3.10 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">813,345</span> (3.10 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurando o Early Stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',         #Monitorando a perda na validação\n",
        "    patience=10,                #Número de épocas sem melhora para interromper o treinamento\n",
        "    verbose=1,                  #Imprime mensagens de parada antecipada\n",
        "    restore_best_weights=True   #Restaura os pesos da melhor época\n",
        ")"
      ],
      "metadata": {
        "id": "j1m7andNI3KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile( loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "JxzfGYJnMOLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código compila o modelo de rede neural convolucional (CNN) usando o otimizador especificado, a função de perda \"binary_crossentropy\" (adequada para problemas de classificação binária) e a métrica \"accuracy\" para avaliar o desempenho durante o treinamento."
      ],
      "metadata": {
        "id": "CjT4ESPKI8BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(base_treino, epochs=100, validation_data=base_teste, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9JFMs7VMoWh",
        "outputId": "5d920882-1aa0-4e59-f86e-459d88896ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 359ms/step - accuracy: 0.4852 - loss: 4.1155 - val_accuracy: 0.5753 - val_loss: 1.3210\n",
            "Epoch 2/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 320ms/step - accuracy: 0.5460 - loss: 2.5421 - val_accuracy: 0.7123 - val_loss: 0.6617\n",
            "Epoch 3/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 176ms/step - accuracy: 0.6214 - loss: 1.3881 - val_accuracy: 0.5068 - val_loss: 0.6652\n",
            "Epoch 4/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 190ms/step - accuracy: 0.6402 - loss: 1.0411 - val_accuracy: 0.6712 - val_loss: 0.6199\n",
            "Epoch 5/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 182ms/step - accuracy: 0.6575 - loss: 0.7260 - val_accuracy: 0.6575 - val_loss: 0.6360\n",
            "Epoch 6/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 177ms/step - accuracy: 0.6545 - loss: 0.8501 - val_accuracy: 0.6301 - val_loss: 0.6791\n",
            "Epoch 7/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 336ms/step - accuracy: 0.6781 - loss: 0.7105 - val_accuracy: 0.7397 - val_loss: 0.6395\n",
            "Epoch 8/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - accuracy: 0.6147 - loss: 0.8157 - val_accuracy: 0.4247 - val_loss: 0.7289\n",
            "Epoch 9/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 174ms/step - accuracy: 0.6370 - loss: 0.8442 - val_accuracy: 0.5479 - val_loss: 0.6555\n",
            "Epoch 10/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 176ms/step - accuracy: 0.6532 - loss: 0.6928 - val_accuracy: 0.4384 - val_loss: 0.7184\n",
            "Epoch 11/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 204ms/step - accuracy: 0.5817 - loss: 0.9465 - val_accuracy: 0.4658 - val_loss: 0.7311\n",
            "Epoch 12/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 298ms/step - accuracy: 0.6888 - loss: 0.6561 - val_accuracy: 0.4521 - val_loss: 0.7057\n",
            "Epoch 13/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 181ms/step - accuracy: 0.5921 - loss: 0.7940 - val_accuracy: 0.5753 - val_loss: 0.7292\n",
            "Epoch 14/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 196ms/step - accuracy: 0.5866 - loss: 0.6992 - val_accuracy: 0.6027 - val_loss: 0.6664\n",
            "Epoch 14: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d9976ee4c10>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Resultado de treinamento da rede neural apresenta as seguintes métricas:\n",
        "\n"
      ],
      "metadata": {
        "id": "bKz3GFmjEDiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **accuracy: 0.7140**: A precisão (accuracy) do modelo no conjunto de treinamento é de aproximadamente 71,40%, indicando que o modelo classificou corretamente cerca de 71% dos exemplos de treinamento.\n",
        "  \n",
        "- **loss: 0.5553**: A função de perda (loss) no conjunto de treinamento é de 0,5553. Esse valor representa o erro do modelo, sugere que o modelo ainda comete alguns erros no treinamento, o que é comum e esperado, pois um valor muito próximo de zero poderia indicar overfitting\n",
        "\n",
        "- **val_accuracy: 0.8082**: A precisão do modelo no conjunto de validação é de 80,82%, o que é superior à precisão do treinamento. Isso sugere que o modelo está generalizando bem para dados não vistos.\n",
        "\n",
        "- **val_loss: 0.5151**: A perda no conjunto de validação é de 0,5151 levemente inferior à perda no conjunto de treinamento. Isso indica que o modelo está conseguindo manter um bom desempenho sem sinais claros de overfitting (ajuste excessivo aos dados de treinamento).\n",
        "\n",
        "Esses resultados mostram que o modelo está com uma performance aceitável e que há espaço para melhorar, mas já possui uma boa capacidade de generalização para dados novos."
      ],
      "metadata": {
        "id": "YgJN11-uFD6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4°Passo: Prevendo novas imagens"
      ],
      "metadata": {
        "id": "KO9A9yjoNGJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_treino.class_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9ereH60O9-g",
        "outputId": "5056af25-842e-4620-dc71-bd7d3c2fc195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bart': 0, 'homer': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image_interface(uploaded_image):\n",
        "    #Converte a imagem carregada para um formato compatível\n",
        "    imagem_teste = Image.fromarray(np.array(uploaded_image))  #Garante que a imagem é compatível com Pillow\n",
        "    imagem_teste = imagem_teste.resize((64, 64))  #Redimensiona para 64x64\n",
        "\n",
        "    #Converte a imagem redimensionada em um array\n",
        "    imagem_teste = image.img_to_array(imagem_teste)  #Converte para array NumPy\n",
        "    imagem_teste /= 255  #Padroniza para valores entre [0, 1]\n",
        "    imagem_teste = np.expand_dims(imagem_teste, axis=0)  #Expande para (1, 64, 64, 3)\n",
        "\n",
        "    #Faz a previsão\n",
        "    previsao = cnn.predict(imagem_teste)[0]\n",
        "\n",
        "    #Retorna o resultado da previsão\n",
        "    return \"Bart\" if previsao < 0.5 else \"Homer\"\n",
        "\n",
        "#Configurando a interface do Gradio\n",
        "interface = gr.Interface(\n",
        "    fn=predict_image_interface,\n",
        "    inputs=\"image\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Classificador Homer e Bart\",\n",
        "    description=\"Envie uma imagem para classificar se é Homer ou Bart\"\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)  #Ativa o modo debug para exibir erros detalhados\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "SECrk3qf_OzT",
        "outputId": "51cda6c4-030c-4a55-910e-a28d4956649f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e97b5ca71d4c440e64.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e97b5ca71d4c440e64.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e97b5ca71d4c440e64.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusão\n"
      ],
      "metadata": {
        "id": "ULJL8cWvF1MY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo apresenta uma boa capacidade de generalização, mas ainda classifica incorretamente algumas imagens quando testado com novos dados. Isso pode indicar que o modelo não capturou completamente todas as variações dos padrões visuais nas classes. A adição de mais dados ou ajustes no treinamento podem ajudar a refinar essa precisão.\n",
        "\n",
        "**Próximos Passos**:\n",
        "\n",
        "1. **Analisar e Identificar Padrões de Erro**:\n",
        "    - Avaliar uma amostra das imagens classificadas incorretamente para verificar se há similaridades ou padrões nas classes confundidas.\n",
        "\n",
        "2. **Expandir e Diversificar os Dados de Treinamento**:\n",
        "   - Se possível, incluir mais imagens representativas das classes que foram classificadas incorretamente.\n",
        "\n",
        "3. **Realizar Fine-Tuning**:\n",
        "   - Retreinar o modelo com as novas imagens ou com um conjunto de dados atualizado pode ajudar a melhorar o reconhecimento em situações previamente problemáticas.\n",
        "\n",
        "4. **Avaliar com Métricas Detalhadas**:\n",
        "   - Gerar uma **matriz de confusão** para ver quais classes são mais frequentemente confundidas. Também analisar precisão, recall e F1-score para ter uma visão mais detalhada do desempenho.\n"
      ],
      "metadata": {
        "id": "KiQtxPPjGDhb"
      }
    }
  ]
}